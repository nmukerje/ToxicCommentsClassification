{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading https://files.pythonhosted.org/packages/33/d1/b1479a770f66d962f545c2101630ce1d5592d90cb4f083d38862e93d16d2/pydot-1.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pydot) (2.2.0)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.1\n",
      "Collecting pydotplus\n",
      "  Downloading https://files.pythonhosted.org/packages/60/bf/62567830b700d9f6930e9ab6831d6ba256f7b0b730acb37278b0ccdffacf/pydotplus-2.0.2.tar.gz (278kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pydotplus) (2.2.0)\n",
      "Building wheels for collected packages: pydotplus\n",
      "  Building wheel for pydotplus (setup.py): started\n",
      "  Building wheel for pydotplus (setup.py): finished with status 'done'\n",
      "  Created wheel for pydotplus: filename=pydotplus-2.0.2-cp36-none-any.whl size=23674 sha256=fa9d5e1fb028e2eec150abc4ea57d84a21a7975853ffd4fa3ed39f6516f58c4d\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/35/7b/ab/66fb7b2ac1f6df87475b09dc48e707b6e0de80a6d8444e3628\n",
      "Successfully built pydotplus\n",
      "Installing collected packages: pydotplus\n",
      "Successfully installed pydotplus-2.0.2\n",
      "Collecting tqdm\n",
      "  Using cached https://files.pythonhosted.org/packages/4a/1c/6359be64e8301b84160f6f6f7936bbfaaa5e9a4eab6cbc681db07600b949/tqdm-4.45.0-py2.py3-none-any.whl\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.45.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "## Dependencies\n",
    "pip install pydot\n",
    "pip install pydotplus\n",
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# helper python file\n",
    "import ml_pipeline as pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T20:42:48.061251Z",
     "start_time": "2020-03-28T20:42:46.019174Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Conv1D\n",
    "from keras.layers import Dropout, Embedding\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3705/153164 [00:00<00:04, 37045.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train:  159571\n",
      "num test:  153164\n",
      "pre-processing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153164/153164 [00:03<00:00, 38560.51it/s]\n",
      "100%|██████████| 159571/159571 [00:04<00:00, 34828.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n",
      "dictionary size:  282101\n"
     ]
    }
   ],
   "source": [
    "(train_df,test_df)=pipeline.read_input_data()\n",
    "(seq_test,seq_train,word_index)=pipeline.tokenize_data(test_df,train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1193514it [07:59, 2488.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 2310\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = 'Glove-Twitter'\n",
    "embedding_matrix=pipeline.prepare_embeddings(EMBEDDING_FILE,word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T20:44:57.038115Z",
     "start_time": "2020-03-28T20:44:57.035042Z"
    }
   },
   "outputs": [],
   "source": [
    "max_features=30000\n",
    "max_seq_len=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "import keras\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Flatten, LSTM\n",
    "from keras.layers import Input, GlobalMaxPool1D, Dropout, SpatialDropout1D, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import concatenate\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_BiLSTM_Conv_Model(embedding_matrix):\n",
    "    # Build Model\n",
    "    inp = Input(shape=(max_seq_len,))\n",
    "\n",
    "    x = Embedding(max_features, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.35)(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.15, recurrent_dropout=0.15))(x)\n",
    "    x = Conv1D(64, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool])\n",
    "\n",
    "    out = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 150, 200)     6000000     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_8 (SpatialDro (None, 150, 200)     0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 150, 256)     336896      spatial_dropout1d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 148, 64)      49216       bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 64)           0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 6)            774         concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,386,886\n",
      "Trainable params: 386,886\n",
      "Non-trainable params: 6,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=init_BiLSTM_Conv_Model(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T22:22:34.307252Z",
     "start_time": "2020-03-28T20:48:01.442037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/5\n",
      "127656/127656 [==============================] - 541s 4ms/step - loss: 0.0627 - acc: 0.9782 - val_loss: 0.0468 - val_acc: 0.9826\n",
      "Epoch 2/5\n",
      "127656/127656 [==============================] - 536s 4ms/step - loss: 0.0482 - acc: 0.9818 - val_loss: 0.0451 - val_acc: 0.9826\n",
      "Epoch 3/5\n",
      "127656/127656 [==============================] - 539s 4ms/step - loss: 0.0452 - acc: 0.9827 - val_loss: 0.0439 - val_acc: 0.9830\n",
      "Epoch 4/5\n",
      "127656/127656 [==============================] - 539s 4ms/step - loss: 0.0434 - acc: 0.9833 - val_loss: 0.0424 - val_acc: 0.9837\n",
      "Epoch 5/5\n",
      "127656/127656 [==============================] - 539s 4ms/step - loss: 0.0415 - acc: 0.9839 - val_loss: 0.0415 - val_acc: 0.9838\n"
     ]
    }
   ],
   "source": [
    "label_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "y_train = train_df[label_names].values\n",
    "history = model.fit(seq_train, y_train, epochs = 5, batch_size = 128, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T22:23:06.819601Z",
     "start_time": "2020-03-28T22:23:06.206576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"Bi-LSTM-Conv-Glove-Preprocessed-model-twitter.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"Bi-LSTM-Conv-Glove-Preprocessed-model-twitter.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T22:39:13.425966Z",
     "start_time": "2020-03-28T22:23:22.618686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 209s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "predictions = model.predict(seq_test, batch_size=128, verbose=1)\n",
    "\n",
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "submission[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] = predictions\n",
    "submission.to_csv('submission-Bi-LSTM-Conv-Glove-Preprocessed-model-twitter.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Labeling\n",
    "\n",
    "1. Merge the predictions on 50% of the test data with the training data.\n",
    "2. Retrain the model on this new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  00001cee341fdb12      1             0        1       0       1   \n",
       "1  0000247867823ef7      0             0        0       0       0   \n",
       "2  00013b17ad220c46      0             0        0       0       0   \n",
       "3  00017563c3f7919a      0             0        0       0       0   \n",
       "4  00017695ad8997eb      0             0        0       0       0   \n",
       "\n",
       "   identity_hate  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalize the submissions\n",
    "submission_norm = submission.copy()\n",
    "submission_norm['toxic'] = submission['toxic'].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "submission_norm['severe_toxic'] = submission['severe_toxic'].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "submission_norm['obscene'] = submission['obscene'].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "submission_norm['threat'] = submission['threat'].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "submission_norm['insult'] = submission['insult'].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "submission_norm['identity_hate'] = submission['identity_hate'].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "submission_norm.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yo bitch ja rule is more succesful then you'll...</td>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from rfc the title is fine as it is imo</td>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sources zawe ashton on lapland</td>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you have a look back at the source the info...</td>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i don't anonymously edit articles at all</td>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text                id  toxic  \\\n",
       "0  yo bitch ja rule is more succesful then you'll...  00001cee341fdb12      1   \n",
       "1            from rfc the title is fine as it is imo  0000247867823ef7      0   \n",
       "2                     sources zawe ashton on lapland  00013b17ad220c46      0   \n",
       "3  if you have a look back at the source the info...  00017563c3f7919a      0   \n",
       "4           i don't anonymously edit articles at all  00017695ad8997eb      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        1       0       1              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine the predictons with the test data to create Pseudo Labeled Data\n",
    "test_x=pd.concat([test_df['comment_text'], submission_norm], axis=1, sort=False)\n",
    "test_x.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take random 50% sample of the DataFrame with replacement\n",
    "test_x=test_x.sample(frac=0.5, replace=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment_text     236153\n",
      "id               236153\n",
      "identity_hate    236153\n",
      "insult           236153\n",
      "obscene          236153\n",
      "severe_toxic     236153\n",
      "threat           236153\n",
      "toxic            236153\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>id</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscene</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>threat</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128037</th>\n",
       "      <td>hey don't worry about it there's all manner of...</td>\n",
       "      <td>d5f8dbc8d5a2138f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>stylistics thanks for your email i'm glad you ...</td>\n",
       "      <td>08bdc11d90732729</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50057</th>\n",
       "      <td>sources for stuff opposed to ayers i found an ...</td>\n",
       "      <td>531f1a43c0816123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109259</th>\n",
       "      <td>there is a probably the most complete to date ...</td>\n",
       "      <td>b64616aa8465aa99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73349</th>\n",
       "      <td>unsourced statement controls no territory in t...</td>\n",
       "      <td>7a3f64a479224faf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text                id  \\\n",
       "128037  hey don't worry about it there's all manner of...  d5f8dbc8d5a2138f   \n",
       "5192    stylistics thanks for your email i'm glad you ...  08bdc11d90732729   \n",
       "50057   sources for stuff opposed to ayers i found an ...  531f1a43c0816123   \n",
       "109259  there is a probably the most complete to date ...  b64616aa8465aa99   \n",
       "73349   unsourced statement controls no territory in t...  7a3f64a479224faf   \n",
       "\n",
       "        identity_hate  insult  obscene  severe_toxic  threat  toxic  \n",
       "128037              0       0        0             0       0      0  \n",
       "5192                0       0        0             0       0      0  \n",
       "50057               0       0        0             0       0      0  \n",
       "109259              0       0        0             0       0      0  \n",
       "73349               0       0        0             0       0      0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge with training dataset\n",
    "combined=pd.concat([test_x,train_df])\n",
    "print (combined.count())\n",
    "combined.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['comment_text']=combined.comment_text.astype(str)\n",
    "combined['comment_text'].fillna(' ')\n",
    "combined_train_y = combined[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
    "combined_train_x = combined['comment_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3558/236153 [00:00<00:06, 35550.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236153/236153 [00:06<00:00, 35703.42it/s]\n",
      "100%|██████████| 153164/153164 [00:03<00:00, 38430.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n",
      "dictionary size:  282101\n"
     ]
    }
   ],
   "source": [
    "(seq_train,seq_test,word_index)=pipeline.tokenize_data(combined,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 150, 200)     6000000     input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_9 (SpatialDro (None, 150, 200)     0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 150, 256)     336896      spatial_dropout1d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 148, 64)      49216       bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 64)           0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 64)           0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128)          0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 6)            774         concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,386,886\n",
      "Trainable params: 386,886\n",
      "Non-trainable params: 6,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=init_BiLSTM_Conv_Model(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 188922 samples, validate on 47231 samples\n",
      "Epoch 1/5\n",
      "188922/188922 [==============================] - 3344s 18ms/step - loss: 0.0830 - acc: 0.9725 - val_loss: 0.0552 - val_acc: 0.9818\n",
      "Epoch 2/5\n",
      "188922/188922 [==============================] - 3336s 18ms/step - loss: 0.0579 - acc: 0.9798 - val_loss: 0.0526 - val_acc: 0.9822\n",
      "Epoch 3/5\n",
      "188922/188922 [==============================] - 3337s 18ms/step - loss: 0.0530 - acc: 0.9813 - val_loss: 0.0529 - val_acc: 0.9824\n",
      "Epoch 4/5\n",
      "188922/188922 [==============================] - 3331s 18ms/step - loss: 0.0502 - acc: 0.9823 - val_loss: 0.0524 - val_acc: 0.9823\n",
      "Epoch 5/5\n",
      "188896/188922 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9830"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "model.fit(seq_train, combined_train_y, batch_size=batch_size, epochs=epochs, verbose=1,validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"Bi-LSTM-Conv-Glove-Preprocessed-model-twitter-pseudolabel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"Bi-LSTM-Conv-Glove-Preprocessed-model-twitter-pseudolabel.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final predictions\n",
    "predictions = model.predict(seq_test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "submission[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] = predictions\n",
    "submission.to_csv('submission-Bi-LSTM-Conv-Glove-Twitter-Plabel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
